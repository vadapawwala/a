Set A
1. Write a python program to implement following classification algorithms
on a given Dataset:
i. Naïve Bayes algorithm (Use Social_Network_Ads.csv)


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

# Naïve Bayes algorithm
naive_bayes_classifier = GaussianNB()
naive_bayes_classifier.fit(X_train_scaled, y_train)
naive_bayes_predictions = naive_bayes_classifier.predict(X_test_scaled)

accuracy = accuracy_score(y_test, naive_bayes_predictions)
classification_rep = classification_report(y_test, naive_bayes_predictions)
confusion_mat = confusion_matrix(y_test, naive_bayes_predictions)

print('Naïve Bayes Accuracy:', accuracy)
print('\nClassification Report for Naïve Bayes:\n', classification_rep)
print('\nConfusion Matrix for Naïve Bayes:\n', confusion_mat)

plt.figure(figsize=(10, 6))

# Plot points for class 'No'
plt.scatter(X_test_scaled[y_test == 0, 0], X_test_scaled[y_test == 0, 1], color='red', label='No (Actual)')
# Plot points for class 'Yes'
plt.scatter(X_test_scaled[y_test == 1, 0], X_test_scaled[y_test == 1, 1], color='green', label='Yes (Actual)')

# Highlight misclassified points
misclassified = y_test != naive_bayes_predictions
plt.scatter(X_test_scaled[misclassified, 0], X_test_scaled[misclassified, 1], color='blue', marker='x', label='Misclassified')

plt.title('Naïve Bayes - Scatter Plot')
plt.xlabel('Scaled Feature 1')
plt.ylabel('Scaled Feature 2')
plt.legend()
plt.show()


1. Write a python program to implement following classification algorithms
on a given Dataset:
ii. Random Forest (Use Social_Network_Ads.csv)

random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest_classifier.fit(X_train_scaled, y_train)
random_forest_predictions = random_forest_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, random_forest_predictions)
classification_rep = classification_report(y_test, random_forest_predictions)
confusion_mat = confusion_matrix(y_test, random_forest_predictions)

print('Random Forest Accuracy:', accuracy)
print('\nClassification Report:\n', classification_rep)
print('\nConfusion Matrix:\n', confusion_mat)

# Scatter diagram for Random Forest
plt.scatter(X_test[:, 0], X_test[:, 1], c=random_forest_predictions, cmap=plt.cm.Paired)
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.title('Random Forest - Scatter Diagram')
plt.show()


1. Write a python program to implement following classification algorithms
on a given Dataset:
iii. Kernel SVM (Use Social_Network_Ads.csv)


kernel_svm_classifier = SVC(kernel='rbf', random_state=42)
kernel_svm_classifier.fit(X_train_scaled, y_train)
kernel_svm_predictions = kernel_svm_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, kernel_svm_predictions)
classification_rep = classification_report(y_test, kernel_svm_predictions)
confusion_mat = confusion_matrix(y_test, kernel_svm_predictions)

print('Kernel SVM Accuracy:', accuracy)
print('\nClassification Report for Kernel SVM:\n', classification_rep)
print('\nConfusion Matrix for Kernel SVM:\n', confusion_mat)

# Scatter plot for Kernel SVM predictions
plt.figure(figsize=(10, 6))

# Plot points for class 'No'
plt.scatter(X_test_scaled[y_test == 0, 0], X_test_scaled[y_test == 0, 1], color='red', label='No (Actual)')
# Plot points for class 'Yes'
plt.scatter(X_test_scaled[y_test == 1, 0], X_test_scaled[y_test == 1, 1], color='green', label='Yes (Actual)')

#misclassified points
misclassified = y_test != kernel_svm_predictions
plt.scatter(X_test_scaled[misclassified, 0], X_test_scaled[misclassified, 1], color='blue', marker='x', label='Misclassified')

h = .02
x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1
y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = kernel_svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

plt.title('Kernel SVM - Scatter Diagram with Decision Boundary')
plt.xlabel('Scaled Feature 1')
plt.ylabel('Scaled Feature 2')
plt.legend()
plt.show()





2. Write a python program to Implement Decision Tree whether or not to play
tennis. (Use Tennis.csv)

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('Tennis.csv')

# Preprocessing: Convert categorical variables to numerical labels
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
for column in data.columns:
    data[column] = label_encoder.fit_transform(data[column])

# Extract features and target variable
X = data.drop(columns=['Play Tennis'])
y = data['Play Tennis']

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Train the Decision Tree classifier
classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(X_train, y_train)

# Make predictions
y_pred = classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

classification_rep = classification_report(y_test, y_pred)
print("Classification Report:\n", classification_rep)

# Visualize the Decision Tree
plt.figure(figsize=(10,7))
fig=plt.figure()

OR


import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('Tennis.csv')

# Preprocessing: Convert categorical variables to numerical labels
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
for column in data.columns:
    data[column] = label_encoder.fit_transform(data[column])

# Extract features and target variable
X = data.drop(columns=['Play Tennis'])
y = data['Play Tennis']

# Splitting the dataset into the Training set and Test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Train the Decision Tree classifier
classifier = DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(X_train, y_train)

# Make predictions
y_pred = classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

classification_rep = classification_report(y_test, y_pred)
print("Classification Report:\n", classification_rep)

# Visualize the Decision Tree
plt.figure(figsize=(15,10))



3. Write a python program to implement k-Nearest Neighbors algorithm to
build a prediction model on a given dataset (Use Social_Network_Ads.csv).

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')

# Select relevant features and target variable
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Feature scaling for better visualization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# k-Nearest Neighbors
k_value = 5  # You can adjust the value of k as needed
knn_classifier = KNeighborsClassifier(n_neighbors=k_value)
knn_classifier.fit(X_train_scaled, y_train)
knn_predictions = knn_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, knn_predictions)
classification_rep = classification_report(y_test, knn_predictions)
confusion_mat = confusion_matrix(y_test, knn_predictions)

print(f'K-Nearest Neighbors (k={k_value}) Accuracy:', accuracy)
print('\nClassification Report for K-Nearest Neighbors:\n', classification_rep)
print('\nConfusion Matrix for K-Nearest Neighbors:\n', confusion_mat)

# Scatter plot for K-Nearest Neighbors predictions
plt.figure(figsize=(10, 6))

# Plot points for class 'No'
plt.scatter(X_test_scaled[y_test == 0, 0], X_test_scaled[y_test == 0, 1], color='red', label='No (Actual)')
# Plot points for class 'Yes'
plt.scatter(X_test_scaled[y_test == 1, 0], X_test_scaled[y_test == 1, 1], color='green', label='Yes (Actual)')

# Highlight misclassified points
misclassified = y_test != knn_predictions
plt.scatter(X_test_scaled[misclassified, 0], X_test_scaled[misclassified, 1], color='blue', marker='x', label='Misclassified')

plt.title(f'K-Nearest Neighbors (k={k_value}) - Scatter Diagram')
plt.xlabel('Scaled Feature 1')
plt.ylabel('Scaled Feature 2')
plt.legend()
plt.show()



Set B (Practice Assignment)
1. Consider the given dataset in the User_Data.csv file:
a) Write a python program to implement k-nearest Neighbors algorithm to
build a prediction model for whether to buy an SUV car or not on a given
dataset.
b) Plot the graph to show classification.
c) Show Accuracy and Precision of the model.



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix

# Load the dataset
dataset = pd.read_csv('User_Data.csv')

# Select relevant features and target variable
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Feature scaling for KNN
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# K-Nearest Neighbors
k_value = 5  # You can adjust the value of k as needed
knn_classifier = KNeighborsClassifier(n_neighbors=k_value)
knn_classifier.fit(X_train_scaled, y_train)
knn_predictions = knn_classifier.predict(X_test_scaled)

# Plot decision boundaries
def plot_decision_boundaries(X, y, classifier, scaler, title):
    h = .02  # step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    Z = classifier.predict(scaler.transform(np.c_[xx.ravel(), yy.ravel()]))
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o')
    plt.xlabel('Age')
    plt.ylabel('Estimated Salary')
    plt.title(title)
    plt.show()

# Plot decision boundaries
plot_decision_boundaries(X_train_scaled, y_train, knn_classifier, scaler, 'K-Nearest Neighbors Decision Boundaries (Training Set)')

# Evaluate the model
accuracy = accuracy_score(y_test, knn_predictions)
precision = precision_score(y_test, knn_predictions)
classification_rep = classification_report(y_test, knn_predictions)
confusion_mat = confusion_matrix(y_test, knn_predictions)

# Display results
print(f'K-Nearest Neighbors (k={k_value}) Accuracy:', accuracy)
print(f'K-Nearest Neighbors (k={k_value}) Precision:', precision)
print('\nClassification Report for K-Nearest Neighbors:\n', classification_rep)
print('\nConfusion Matrix for K-Nearest Neighbors:\n', confusion_mat)






from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt


weather = ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy']
temp = ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild']
humidity = ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High']
windy = [False, True, False, False, False, True, True, False, False, False, True, True, False, True]
play_tennis = ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']

weather_encoded = [0 if w == 'Sunny' else 1 if w == 'Overcast' else 2 for w in weather]
temp_encoded = [0 if t == 'Hot' else 1 if t == 'Mild' else 2 for t in temp]
humidity_encoded = [0 if h == 'High' else 1 for h in humidity]
windy_encoded = [0 if w == False else 1 for w in windy]


X = list(zip(weather_encoded, temp_encoded, humidity_encoded, windy_encoded))


y = [0 if p == 'No' else 1 for p in play_tennis]


clf = DecisionTreeClassifier()
clf = clf.fit(X, y)


plt.figure(figsize=(10, 7))
plt.scatter(weather_encoded, temp_encoded, c=y, cmap='viridis', s=100)
plt.xlabel('Weather')
plt.ylabel('Temperature')
plt.title('Scatter Plot of Weather vs Temperature')
plt.colorbar(label='Play Tennis')
plt.show()

